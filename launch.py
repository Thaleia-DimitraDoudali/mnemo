############# MNEMO ##############
## Author: Thaleia Dimitra Doudali
##################################

import sys, csv

#### Mnemo's software components #######
import sensitivity_engine as sensitivity
import pattern_engine as pattern
import estimate_engine as estimate

#### Mnemo's Input: appropriately modify the conf.csv file####
def read_input(file):
    reader = csv.reader(open(file, 'r'))
    d = {}
    for row in reader:
        k, v = row
        d[k] = v
    return d

def write_output(data):
    with open("output.txt", "wb") as csv_file:
        writer = csv.writer(csv_file, delimiter=',')
        for row in data:
            writer.writerow(row)

if __name__ == "__main__":

    # Read configuration file
    conf = read_input(sys.argv[1])

    # Step 1: Feed into the Sensitivity Engine the input workload, in order to get the performance baselines for the all-data-in-fast and all-data-in-slow memory cases.
    fast_base, slow_base = sensitivity.main(conf['Name'], conf['FastIP'], conf['FastPort'], conf['SlowIP'], conf['SlowPort'], conf['KeyFile'], conf['ReqFile'], conf['MaxKeyID'], conf['RequestCount'])

    # Step 2: Feed into the Pattern Engine, the workload related input and get the request access pattern as a dictionary
    reqs_dict = pattern.main(conf['KeyFile'], conf['ReqFile'], conf['MaxKeyID'])

    #Step 3: Feed into the Estimate Engine the data generated by the Sensitivity and Pattern Engines, and decide if you want data to represented raw or as a slowdown from all-data-in-fast.
    output = estimate.main(conf['Price'], reqs_dict, fast_base, slow_base, conf['MaxKeyID'], conf['Normalize'])

    # Populate raw result file
    write_output(output)
